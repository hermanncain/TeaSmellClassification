{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# overview:\n",
    "7 classes of tea\n",
    "8 features from 8 smell sensors\n",
    "100- frames sampling from sensors\n",
    "\n",
    "total number of samples: 59, with 7 class of 9,9,9,9,9,9,5 samples\n",
    "\n",
    "'''\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# teaType = ['Luan','Huangshan','Taiping','Maoshan','Dinggu','Queshe','Biluochun']\n",
    "\n",
    "\n",
    "def loadData(tensor=False,onehot=False,normalized=False,separated=False):\n",
    "    tea_data = []\n",
    "    tea_label = []   \n",
    "    for i in range(7):\n",
    "        fname = str(i+1)+'.csv'\n",
    "        train = pd.read_csv(open(fname,'r'))\n",
    "        temp_data = []\n",
    "        for j in range(9):\n",
    "            label = i\n",
    "            tempdata = train.iloc[0:95,9*j:9*j+8].values # remove nan frames and crop data to the same length\n",
    "            if tempdata.shape[1] != 0:\n",
    "                # data augmentation\n",
    "                for k in range(16):\n",
    "                    data = tempdata[k:k+80]\n",
    "                    if normalized:\n",
    "                        scaler = StandardScaler()\n",
    "                        data = scaler.fit_transform(data)\n",
    "                    if separated:\n",
    "                        for x in data:\n",
    "                            if tensor:\n",
    "                                tea_data.append(x)\n",
    "                            else:\n",
    "                                tea_data.append(x.reshape(1,-1)[0])\n",
    "                            tea_label.append(label)\n",
    "                    else:\n",
    "                        if tensor:\n",
    "                            tea_data.append(data)\n",
    "                        else:\n",
    "                            tea_data.append(data.reshape(1,-1)[0])\n",
    "                        tea_label.append(label)\n",
    "    if onehot:\n",
    "        enc = OneHotEncoder()\n",
    "        tea_label = enc.fit_transform(np.array(tea_label).reshape(-1,1)).toarray()\n",
    "    return train_test_split(tea_data,tea_label,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'poly', 'C': 2.0} 0.8145695364238411\n"
     ]
    }
   ],
   "source": [
    "# svm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "trainX,testX,trainY,testY = loadData()\n",
    "\n",
    "svmparams = {'kernel':('linear','rbf','poly'),'C':[0.1,1.0,2.0,5.0,10.0]}\n",
    "svmclf = SVC()\n",
    "gsearch1 = GridSearchCV(svmclf,svmparams,scoring='accuracy',cv=3)\n",
    "gsearch1.fit(trainX,trainY)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "\n",
    "# normalized data lead to worse result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7936507936507936\n"
     ]
    }
   ],
   "source": [
    "svmclfr = SVC(kernel='poly',C=2.0)\n",
    "svmclfr.fit(trainX,trainY)\n",
    "print(metrics.accuracy_score(testY,svmclfr.predict(testX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 40} 0.7973509933774835\n"
     ]
    }
   ],
   "source": [
    "# rf\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "trainX,testX,trainY,testY = loadData()\n",
    "\n",
    "forestparams1 = {'n_estimators':range(10,200,10)}\n",
    "forestclf1 = RandomForestClassifier(random_state=10)\n",
    "gsearch1 = GridSearchCV(forestclf1,forestparams1,scoring='accuracy',cv=3)\n",
    "gsearch1.fit(trainX,trainY)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'min_samples_split': 8} 0.8119205298013245\n"
     ]
    }
   ],
   "source": [
    "forestparams2 = {'max_depth':range(2,14,2), 'min_samples_split':range(2,10,1)}\n",
    "forestclf2 = RandomForestClassifier(n_estimators=40,random_state=10, oob_score=True)\n",
    "gsearch2 = GridSearchCV(forestclf2,forestparams2,scoring='accuracy',cv=3)\n",
    "gsearch2.fit(trainX,trainY)\n",
    "print(gsearch2.best_params_, gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783068783068783\n"
     ]
    }
   ],
   "source": [
    "forestclfr = RandomForestClassifier(n_estimators=50,random_state=10, min_samples_split=8, max_depth=6, oob_score=True)\n",
    "forestclfr.fit(trainX,trainY)\n",
    "print(metrics.accuracy_score(testY,forestclfr.predict(testX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'logistic', 'hidden_layer_sizes': (100, 30)} 0.5655629139072847\n"
     ]
    }
   ],
   "source": [
    "# mlp\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "trainX,testX,trainY,testY = loadData(normalized=True)\n",
    "\n",
    "mlpparams1 = {'hidden_layer_sizes':((100,30),(80,80,20),(64,32,8),(50,20),(40,30,20,10)),'activation':('relu','logistic','tanh')}\n",
    "mlpclf1 = MLPClassifier(solver='lbfgs', random_state=1)\n",
    "gsearch1 = GridSearchCV(mlpclf1,mlpparams1,scoring='accuracy',cv=3)\n",
    "gsearch1.fit(trainX,trainY)\n",
    "print(gsearch1.best_params_, gsearch1.best_score_)\n",
    "# underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001} 0.5655629139072847\n"
     ]
    }
   ],
   "source": [
    "mlpparams2 = {'alpha':(0.0001,0.001,0.01,0.1,1,10)}\n",
    "mlpclf2 = MLPClassifier(hidden_layer_sizes=(100, 30),activation='logistic',solver='lbfgs', random_state=1)\n",
    "gsearch2 = GridSearchCV(mlpclf2,mlpparams2,scoring='accuracy',cv=3)\n",
    "gsearch2.fit(trainX,trainY)\n",
    "print(gsearch2.best_params_, gsearch2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5925925925925926\n"
     ]
    }
   ],
   "source": [
    "mlpclfr = MLPClassifier(hidden_layer_sizes=(100,20),solver='lbfgs',activation='logistic', alpha=0.0001,random_state=1)\n",
    "mlpclfr.fit(trainX,trainY)\n",
    "print(metrics.accuracy_score(testY,mlpclfr.predict(testX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-f44998dd6055>:41: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import  rnn\n",
    "\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "epoch = 300\n",
    "n_inputs = 8\n",
    "n_steps = 80\n",
    "n_hidden_units = 16\n",
    "n_classes = 7\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = {\n",
    "    'in': tf.get_variable('in', shape=[n_inputs, n_hidden_units], initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'out': tf.get_variable('out', shape=[n_hidden_units, n_classes], initializer=tf.contrib.layers.xavier_initializer()),\n",
    "}\n",
    "biases = {\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}\n",
    "\n",
    "def RNN(X, weights, biases):\n",
    "\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "    results = tf.matmul(final_state[1], weights['out']) + biases['out']\n",
    "    \n",
    "    return results\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "    \n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0/300:  [0.125, 2.0357413]\n",
      "train 1/300:  [0.15625, 2.0019906]\n",
      "train 2/300:  [0.15625, 1.9691298]\n",
      "train 3/300:  [0.15625, 1.9394873]\n",
      "train 4/300:  [0.15625, 1.9148144]\n",
      "train 5/300:  [0.1875, 1.8940129]\n",
      "train 6/300:  [0.25, 1.8754876]\n",
      "train 7/300:  [0.25, 1.8583013]\n",
      "train 8/300:  [0.25, 1.8418925]\n",
      "train 9/300:  [0.25, 1.8258897]\n",
      "train 10/300:  [0.25, 1.810071]\n",
      "train 11/300:  [0.28125, 1.794323]\n",
      "train 12/300:  [0.3125, 1.778597]\n",
      "train 13/300:  [0.3125, 1.7628777]\n",
      "train 14/300:  [0.34375, 1.7471552]\n",
      "train 15/300:  [0.3125, 1.7314084]\n",
      "train 16/300:  [0.34375, 1.7156097]\n",
      "train 17/300:  [0.34375, 1.6997384]\n",
      "train 18/300:  [0.375, 1.68379]\n",
      "train 19/300:  [0.375, 1.6677814]\n",
      "train 20/300:  [0.375, 1.6517513]\n",
      "train 21/300:  [0.40625, 1.6357563]\n",
      "train 22/300:  [0.5, 1.6198637]\n",
      "train 23/300:  [0.46875, 1.6041423]\n",
      "train 24/300:  [0.53125, 1.5886499]\n",
      "train 25/300:  [0.53125, 1.5734253]\n",
      "train 26/300:  [0.59375, 1.5584843]\n",
      "train 27/300:  [0.59375, 1.5438225]\n",
      "train 28/300:  [0.59375, 1.5294195]\n",
      "train 29/300:  [0.625, 1.5152473]\n",
      "train 30/300:  [0.65625, 1.5012767]\n",
      "train 31/300:  [0.65625, 1.4874814]\n",
      "train 32/300:  [0.59375, 1.4738414]\n",
      "train 33/300:  [0.59375, 1.4603442]\n",
      "train 34/300:  [0.59375, 1.4469836]\n",
      "train 35/300:  [0.59375, 1.433759]\n",
      "train 36/300:  [0.59375, 1.4206746]\n",
      "train 37/300:  [0.59375, 1.4077358]\n",
      "train 38/300:  [0.59375, 1.3949493]\n",
      "train 39/300:  [0.59375, 1.3823197]\n",
      "train 40/300:  [0.59375, 1.369849]\n",
      "train 41/300:  [0.59375, 1.3575355]\n",
      "train 42/300:  [0.59375, 1.3453717]\n",
      "train 43/300:  [0.625, 1.3333471]\n",
      "train 44/300:  [0.625, 1.3214508]\n",
      "train 45/300:  [0.625, 1.3096764]\n",
      "train 46/300:  [0.625, 1.298022]\n",
      "train 47/300:  [0.625, 1.2864859]\n",
      "train 48/300:  [0.625, 1.275059]\n",
      "train 49/300:  [0.625, 1.2637173]\n",
      "train 50/300:  [0.625, 1.2524272]\n",
      "train 51/300:  [0.625, 1.2411711]\n",
      "train 52/300:  [0.625, 1.2299538]\n",
      "train 53/300:  [0.625, 1.2187811]\n",
      "train 54/300:  [0.625, 1.207651]\n",
      "train 55/300:  [0.625, 1.1965611]\n",
      "train 56/300:  [0.625, 1.1855127]\n",
      "train 57/300:  [0.625, 1.1745105]\n",
      "train 58/300:  [0.625, 1.1635636]\n",
      "train 59/300:  [0.625, 1.1526839]\n",
      "train 60/300:  [0.625, 1.1418841]\n",
      "train 61/300:  [0.625, 1.1311667]\n",
      "train 62/300:  [0.625, 1.1205128]\n",
      "train 63/300:  [0.625, 1.1099073]\n",
      "train 64/300:  [0.625, 1.099513]\n",
      "train 65/300:  [0.65625, 1.0898767]\n",
      "train 66/300:  [0.65625, 1.0811939]\n",
      "train 67/300:  [0.65625, 1.0726244]\n",
      "train 68/300:  [0.65625, 1.0634898]\n",
      "train 69/300:  [0.6875, 1.0535355]\n",
      "train 70/300:  [0.6875, 1.0425435]\n",
      "train 71/300:  [0.6875, 1.0290902]\n",
      "train 72/300:  [0.6875, 1.0174794]\n",
      "train 73/300:  [0.6875, 1.0069642]\n",
      "train 74/300:  [0.6875, 0.9963876]\n",
      "train 75/300:  [0.6875, 0.98583364]\n",
      "train 76/300:  [0.6875, 0.97559726]\n",
      "train 77/300:  [0.6875, 0.96559215]\n",
      "train 78/300:  [0.6875, 0.9551979]\n",
      "train 79/300:  [0.6875, 0.9452684]\n",
      "train 80/300:  [0.6875, 0.9348427]\n",
      "train 81/300:  [0.6875, 0.9251547]\n",
      "train 82/300:  [0.6875, 0.9155524]\n",
      "train 83/300:  [0.6875, 0.90586233]\n",
      "train 84/300:  [0.6875, 0.89358956]\n",
      "train 85/300:  [0.6875, 0.88297474]\n",
      "train 86/300:  [0.6875, 0.87401056]\n",
      "train 87/300:  [0.71875, 0.86580855]\n",
      "train 88/300:  [0.71875, 0.85764277]\n",
      "train 89/300:  [0.71875, 0.8515203]\n",
      "train 90/300:  [0.75, 0.8437005]\n",
      "train 91/300:  [0.75, 0.8362998]\n",
      "train 92/300:  [0.75, 0.8279614]\n",
      "train 93/300:  [0.75, 0.8195276]\n",
      "train 94/300:  [0.75, 0.8137426]\n",
      "train 95/300:  [0.75, 0.80768895]\n",
      "train 96/300:  [0.75, 0.80006087]\n",
      "train 97/300:  [0.75, 0.7930486]\n",
      "train 98/300:  [0.75, 0.7861172]\n",
      "train 99/300:  [0.75, 0.780136]\n",
      "train 100/300:  [0.78125, 0.7726698]\n",
      "train 101/300:  [0.78125, 0.76828516]\n",
      "train 102/300:  [0.78125, 0.7616319]\n",
      "train 103/300:  [0.78125, 0.75749576]\n",
      "train 104/300:  [0.78125, 0.7517137]\n",
      "train 105/300:  [0.8125, 0.7472223]\n",
      "train 106/300:  [0.84375, 0.7416694]\n",
      "train 107/300:  [0.8125, 0.7356313]\n",
      "train 108/300:  [0.8125, 0.73245025]\n",
      "train 109/300:  [0.8125, 0.7271973]\n",
      "train 110/300:  [0.8125, 0.72099173]\n",
      "train 111/300:  [0.8125, 0.7171659]\n",
      "train 112/300:  [0.8125, 0.711191]\n",
      "train 113/300:  [0.8125, 0.7079298]\n",
      "train 114/300:  [0.8125, 0.7029848]\n",
      "train 115/300:  [0.8125, 0.6987244]\n",
      "train 116/300:  [0.8125, 0.6938031]\n",
      "train 117/300:  [0.8125, 0.6893766]\n",
      "train 118/300:  [0.8125, 0.68431175]\n",
      "train 119/300:  [0.8125, 0.6784036]\n",
      "train 120/300:  [0.8125, 0.67292666]\n",
      "train 121/300:  [0.8125, 0.663204]\n",
      "train 122/300:  [0.8125, 0.65564454]\n",
      "train 123/300:  [0.78125, 0.65183884]\n",
      "train 124/300:  [0.78125, 0.64879113]\n",
      "train 125/300:  [0.78125, 0.643719]\n",
      "train 126/300:  [0.78125, 0.64027834]\n",
      "train 127/300:  [0.78125, 0.6389925]\n",
      "train 128/300:  [0.78125, 0.6352682]\n",
      "train 129/300:  [0.78125, 0.6313169]\n",
      "train 130/300:  [0.78125, 0.6279556]\n",
      "train 131/300:  [0.78125, 0.6267445]\n",
      "train 132/300:  [0.78125, 0.62236995]\n",
      "train 133/300:  [0.78125, 0.6202726]\n",
      "train 134/300:  [0.78125, 0.61814415]\n",
      "train 135/300:  [0.78125, 0.6147742]\n",
      "train 136/300:  [0.78125, 0.6115054]\n",
      "train 137/300:  [0.78125, 0.6083692]\n",
      "train 138/300:  [0.8125, 0.60546386]\n",
      "train 139/300:  [0.8125, 0.6027903]\n",
      "train 140/300:  [0.8125, 0.598961]\n",
      "train 141/300:  [0.8125, 0.59562707]\n",
      "train 142/300:  [0.78125, 0.59439754]\n",
      "train 143/300:  [0.8125, 0.59151644]\n",
      "train 144/300:  [0.8125, 0.58716404]\n",
      "train 145/300:  [0.8125, 0.5838419]\n",
      "train 146/300:  [0.8125, 0.5801673]\n",
      "train 147/300:  [0.8125, 0.5769534]\n",
      "train 148/300:  [0.8125, 0.5738685]\n",
      "train 149/300:  [0.8125, 0.57085425]\n",
      "train 150/300:  [0.8125, 0.5676626]\n",
      "train 151/300:  [0.8125, 0.5649028]\n",
      "train 152/300:  [0.78125, 0.5626128]\n",
      "train 153/300:  [0.78125, 0.56258875]\n",
      "train 154/300:  [0.75, 0.5592295]\n",
      "train 155/300:  [0.75, 0.5628973]\n",
      "train 156/300:  [0.75, 0.5532916]\n",
      "train 157/300:  [0.78125, 0.5540477]\n",
      "train 158/300:  [0.75, 0.55925775]\n",
      "train 159/300:  [0.75, 0.55589306]\n",
      "train 160/300:  [0.78125, 0.55067515]\n",
      "train 161/300:  [0.75, 0.5498925]\n",
      "train 162/300:  [0.75, 0.54892933]\n",
      "train 163/300:  [0.75, 0.5454385]\n",
      "train 164/300:  [0.75, 0.5443167]\n",
      "train 165/300:  [0.75, 0.5408769]\n",
      "train 166/300:  [0.75, 0.53859854]\n",
      "train 167/300:  [0.75, 0.5362769]\n",
      "train 168/300:  [0.75, 0.5341834]\n",
      "train 169/300:  [0.75, 0.5319977]\n",
      "train 170/300:  [0.75, 0.5299981]\n",
      "train 171/300:  [0.75, 0.52796745]\n",
      "train 172/300:  [0.75, 0.5260205]\n",
      "train 173/300:  [0.75, 0.5239713]\n",
      "train 174/300:  [0.75, 0.52199596]\n",
      "train 175/300:  [0.75, 0.5200833]\n",
      "train 176/300:  [0.75, 0.5181217]\n",
      "train 177/300:  [0.75, 0.51618654]\n",
      "train 178/300:  [0.75, 0.5142862]\n",
      "train 179/300:  [0.6875, 0.6237216]\n",
      "train 180/300:  [0.71875, 0.5339966]\n",
      "train 181/300:  [0.71875, 0.5273111]\n",
      "train 182/300:  [0.71875, 0.5213429]\n",
      "train 183/300:  [0.75, 0.51804733]\n",
      "train 184/300:  [0.75, 0.5162867]\n",
      "train 185/300:  [0.75, 0.51593626]\n",
      "train 186/300:  [0.75, 0.5141068]\n",
      "train 187/300:  [0.75, 0.5122094]\n",
      "train 188/300:  [0.75, 0.5105686]\n",
      "train 189/300:  [0.75, 0.50909185]\n",
      "train 190/300:  [0.75, 0.5076641]\n",
      "train 191/300:  [0.75, 0.5062915]\n",
      "train 192/300:  [0.75, 0.5049449]\n",
      "train 193/300:  [0.75, 0.50362974]\n",
      "train 194/300:  [0.75, 0.50233924]\n",
      "train 195/300:  [0.75, 0.501071]\n",
      "train 196/300:  [0.75, 0.4998222]\n",
      "train 197/300:  [0.75, 0.49859095]\n",
      "train 198/300:  [0.75, 0.4973759]\n",
      "train 199/300:  [0.75, 0.4961757]\n",
      "train 200/300:  [0.75, 0.49498957]\n",
      "train 201/300:  [0.75, 0.49381712]\n",
      "train 202/300:  [0.75, 0.49265823]\n",
      "train 203/300:  [0.75, 0.49151358]\n",
      "train 204/300:  [0.75, 0.4903856]\n",
      "train 205/300:  [0.75, 0.48928213]\n",
      "train 206/300:  [0.75, 0.4882395]\n",
      "train 207/300:  [0.75, 0.48739842]\n",
      "train 208/300:  [0.75, 0.48609936]\n",
      "train 209/300:  [0.75, 0.48476964]\n",
      "train 210/300:  [0.75, 0.48365688]\n",
      "train 211/300:  [0.75, 0.48250756]\n",
      "train 212/300:  [0.75, 0.4814227]\n",
      "train 213/300:  [0.75, 0.48030818]\n",
      "train 214/300:  [0.75, 0.47919658]\n",
      "train 215/300:  [0.75, 0.47807974]\n",
      "train 216/300:  [0.75, 0.47696877]\n",
      "train 217/300:  [0.75, 0.47585952]\n",
      "train 218/300:  [0.75, 0.47475725]\n",
      "train 219/300:  [0.75, 0.47366524]\n",
      "train 220/300:  [0.75, 0.47258902]\n",
      "train 221/300:  [0.75, 0.471529]\n",
      "train 222/300:  [0.75, 0.47048452]\n",
      "train 223/300:  [0.75, 0.4694516]\n",
      "train 224/300:  [0.75, 0.46842742]\n",
      "train 225/300:  [0.75, 0.46740848]\n",
      "train 226/300:  [0.75, 0.46639302]\n",
      "train 227/300:  [0.75, 0.46537954]\n",
      "train 228/300:  [0.75, 0.4643679]\n",
      "train 229/300:  [0.75, 0.46335828]\n",
      "train 230/300:  [0.75, 0.46235344]\n",
      "train 231/300:  [0.75, 0.4613581]\n",
      "train 232/300:  [0.75, 0.46037424]\n",
      "train 233/300:  [0.75, 0.45939937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 234/300:  [0.75, 0.45843172]\n",
      "train 235/300:  [0.75, 0.45746976]\n",
      "train 236/300:  [0.75, 0.4565127]\n",
      "train 237/300:  [0.75, 0.45556012]\n",
      "train 238/300:  [0.75, 0.45461145]\n",
      "train 239/300:  [0.75, 0.45366666]\n",
      "train 240/300:  [0.75, 0.4527254]\n",
      "train 241/300:  [0.75, 0.45178747]\n",
      "train 242/300:  [0.75, 0.45085257]\n",
      "train 243/300:  [0.75, 0.44992042]\n",
      "train 244/300:  [0.75, 0.44899058]\n",
      "train 245/300:  [0.75, 0.448063]\n",
      "train 246/300:  [0.75, 0.44713748]\n",
      "train 247/300:  [0.75, 0.44621402]\n",
      "train 248/300:  [0.75, 0.44529292]\n",
      "train 249/300:  [0.75, 0.44437438]\n",
      "train 250/300:  [0.75, 0.44345802]\n",
      "train 251/300:  [0.75, 0.4425453]\n",
      "train 252/300:  [0.75, 0.44163412]\n",
      "train 253/300:  [0.75, 0.44072688]\n",
      "train 254/300:  [0.75, 0.4398219]\n",
      "train 255/300:  [0.75, 0.4389173]\n",
      "train 256/300:  [0.71875, 0.4380188]\n",
      "train 257/300:  [0.71875, 0.43711418]\n",
      "train 258/300:  [0.71875, 0.43620765]\n",
      "train 259/300:  [0.71875, 0.43528464]\n",
      "train 260/300:  [0.71875, 0.43438828]\n",
      "train 261/300:  [0.71875, 0.43342698]\n",
      "train 262/300:  [0.71875, 0.43256655]\n",
      "train 263/300:  [0.71875, 0.4315732]\n",
      "train 264/300:  [0.71875, 0.43072233]\n",
      "train 265/300:  [0.71875, 0.429778]\n",
      "train 266/300:  [0.71875, 0.42962867]\n",
      "train 267/300:  [0.71875, 0.42883667]\n",
      "train 268/300:  [0.71875, 0.4939976]\n",
      "train 269/300:  [0.71875, 0.43244937]\n",
      "train 270/300:  [0.75, 0.42819458]\n",
      "train 271/300:  [0.71875, 0.4277826]\n",
      "train 272/300:  [0.71875, 0.42857414]\n",
      "train 273/300:  [0.71875, 0.42921615]\n",
      "train 274/300:  [0.71875, 0.42741165]\n",
      "train 275/300:  [0.71875, 0.42731994]\n",
      "train 276/300:  [0.71875, 0.42710775]\n",
      "train 277/300:  [0.71875, 0.42668843]\n",
      "train 278/300:  [0.71875, 0.4262498]\n",
      "train 279/300:  [0.71875, 0.42577738]\n",
      "train 280/300:  [0.71875, 0.42522198]\n",
      "train 281/300:  [0.71875, 0.42455137]\n",
      "train 282/300:  [0.71875, 0.4240375]\n",
      "train 283/300:  [0.71875, 0.4234774]\n",
      "train 284/300:  [0.71875, 0.4229138]\n",
      "train 285/300:  [0.71875, 0.42228532]\n",
      "train 286/300:  [0.71875, 0.4218629]\n",
      "train 287/300:  [0.71875, 0.42145538]\n",
      "train 288/300:  [0.71875, 0.42087448]\n",
      "train 289/300:  [0.71875, 0.42024112]\n",
      "train 290/300:  [0.71875, 0.4196165]\n",
      "train 291/300:  [0.71875, 0.41900182]\n",
      "train 292/300:  [0.71875, 0.41839376]\n",
      "train 293/300:  [0.71875, 0.4177909]\n",
      "train 294/300:  [0.71875, 0.41719183]\n",
      "train 295/300:  [0.71875, 0.41659594]\n",
      "train 296/300:  [0.71875, 0.4160024]\n",
      "train 297/300:  [0.71875, 0.4154106]\n",
      "train 298/300:  [0.71875, 0.41482013]\n",
      "train 299/300:  [0.71875, 0.4142307]\n",
      "test:  0.8017578125\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_x,test_x,train_y,test_y = loadData(onehot=True,tensor=True,normalized=True)\n",
    "    batch_num = len(train_x)//batch_size\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(epoch):\n",
    "        for i in range(batch_num):\n",
    "            batch_xs = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_ys = train_y[i*batch_size:(i+1)*batch_size]\n",
    "            sess.run([train_op], feed_dict={x: batch_xs,y: batch_ys,})\n",
    "            if i == batch_num -1:\n",
    "                print('train %d/%d: '%(step,epoch), sess.run([accuracy,cost], feed_dict={x: batch_xs,y: batch_ys,}))\n",
    "    acc = 0\n",
    "    batch_num = len(test_x)//batch_size\n",
    "    for i in range(batch_num):\n",
    "        batch_xs = test_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_ys = test_y[i*batch_size:(i+1)*batch_size]\n",
    "        acc = (acc + sess.run(accuracy, feed_dict={x:batch_xs,y:batch_ys}))/2\n",
    "    print('test: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3819ed35ef60>:51: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:397: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:146: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "batch_size = 16\n",
    "epoch = 500\n",
    "\n",
    "train_x,test_x,train_y,test_y = loadData(onehot=True,tensor=True,normalized=True)\n",
    "batch_num = len(train_x)//batch_size\n",
    "\n",
    "def conv1d(x,out_c,name='conv1d'):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w',[5,x.get_shape()[-1],out_c],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                           regularizer = tf.contrib.layers.l2_regularizer(0.1))\n",
    "        conv = tf.nn.conv1d(x,w,stride=4,padding='SAME')\n",
    "        b = tf.get_variable('b',[out_c],initializer=tf.constant_initializer(0.0))\n",
    "        #conv = tf.reshape(tf.nn.bias_add(conv,b),conv.get_shape())\n",
    "    return tf.add(conv,b)\n",
    "\n",
    "def pool1d(x):\n",
    "    return tf.nn.pool(x,window_shape=[4],strides=[3],pooling_type='MAX',padding='SAME')\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 80, 8])\n",
    "y = tf.placeholder(tf.float32, [None, 7])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def CNN(x):\n",
    "    # 80 x 8\n",
    "    net = tf.nn.relu(tf.contrib.layers.batch_norm(conv1d(x,16,name='conv1')))\n",
    "    # 20 x 16\n",
    "    net = pool1d(net)\n",
    "    # 7 x 16\n",
    "    net = tf.nn.relu(tf.contrib.layers.batch_norm(conv1d(net,32,name='conv2')))\n",
    "    # 2 x 32\n",
    "    net = pool1d(net)\n",
    "    # 1 x 32\n",
    "    net = slim.flatten(net)\n",
    "    net = slim.fully_connected(net,16)\n",
    "    net = slim.dropout(net,keep_prob)\n",
    "    net = slim.fully_connected(net,7)\n",
    "    \n",
    "    return net\n",
    "\n",
    "pred = CNN(x)\n",
    "\n",
    "#global_step = tf.Variable(0)\n",
    "#lr = tf.train.exponential_decay(1e-4, global_step, batch_num, 0.96, staircase=True)\n",
    "\n",
    "loss = slim.losses.softmax_cross_entropy(pred, y)\n",
    "op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0/500:  [0.125, 1.939223]\n",
      "train 1/500:  [0.1875, 1.8997973]\n",
      "train 2/500:  [0.1875, 1.8687615]\n",
      "train 3/500:  [0.375, 1.8038554]\n",
      "train 4/500:  [0.4375, 1.7858467]\n",
      "train 5/500:  [0.5625, 1.7152777]\n",
      "train 6/500:  [0.5625, 1.6712673]\n",
      "train 7/500:  [0.5625, 1.637326]\n",
      "train 8/500:  [0.5625, 1.5189373]\n",
      "train 9/500:  [0.5625, 1.4436007]\n",
      "train 10/500:  [0.5625, 1.4394403]\n",
      "train 11/500:  [0.625, 1.3627715]\n",
      "train 12/500:  [0.625, 1.2818942]\n",
      "train 13/500:  [0.625, 1.2383484]\n",
      "train 14/500:  [0.6875, 1.180142]\n",
      "train 15/500:  [0.8125, 1.1107907]\n",
      "train 16/500:  [0.8125, 1.0571201]\n",
      "train 17/500:  [0.75, 1.0051222]\n",
      "train 18/500:  [0.8125, 0.9657849]\n",
      "train 19/500:  [0.75, 0.9526783]\n",
      "train 20/500:  [0.6875, 0.92737615]\n",
      "train 21/500:  [0.75, 0.896422]\n",
      "train 22/500:  [0.75, 0.8548657]\n",
      "train 23/500:  [0.75, 0.8114078]\n",
      "train 24/500:  [0.75, 0.80449986]\n",
      "train 25/500:  [0.8125, 0.76068234]\n",
      "train 26/500:  [0.8125, 0.71415603]\n",
      "train 27/500:  [0.8125, 0.6904806]\n",
      "train 28/500:  [0.75, 0.7035519]\n",
      "train 29/500:  [0.8125, 0.68898]\n",
      "train 30/500:  [0.8125, 0.6189064]\n",
      "train 31/500:  [0.8125, 0.6127609]\n",
      "train 32/500:  [0.8125, 0.5861747]\n",
      "train 33/500:  [0.8125, 0.5930568]\n",
      "train 34/500:  [0.8125, 0.55964655]\n",
      "train 35/500:  [0.8125, 0.5279469]\n",
      "train 36/500:  [0.8125, 0.49883235]\n",
      "train 37/500:  [0.8125, 0.51123655]\n",
      "train 38/500:  [0.8125, 0.46200067]\n",
      "train 39/500:  [0.8125, 0.4475287]\n",
      "train 40/500:  [0.8125, 0.4249528]\n",
      "train 41/500:  [0.8125, 0.44949955]\n",
      "train 42/500:  [0.8125, 0.41790918]\n",
      "train 43/500:  [0.8125, 0.3980912]\n",
      "train 44/500:  [0.8125, 0.39248604]\n",
      "train 45/500:  [0.8125, 0.37299308]\n",
      "train 46/500:  [0.8125, 0.3607329]\n",
      "train 47/500:  [0.75, 0.37746]\n",
      "train 48/500:  [0.75, 0.3661016]\n",
      "train 49/500:  [0.8125, 0.34687313]\n",
      "train 50/500:  [0.8125, 0.34762004]\n",
      "train 51/500:  [0.8125, 0.33559525]\n",
      "train 52/500:  [0.75, 0.3494494]\n",
      "train 53/500:  [0.8125, 0.34222943]\n",
      "train 54/500:  [0.8125, 0.33867362]\n",
      "train 55/500:  [0.75, 0.3432523]\n",
      "train 56/500:  [0.75, 0.34686232]\n",
      "train 57/500:  [0.75, 0.33136988]\n",
      "train 58/500:  [0.8125, 0.31975335]\n",
      "train 59/500:  [0.75, 0.3182726]\n",
      "train 60/500:  [0.8125, 0.31217855]\n",
      "train 61/500:  [0.75, 0.29970977]\n",
      "train 62/500:  [0.8125, 0.2928545]\n",
      "train 63/500:  [0.75, 0.30231613]\n",
      "train 64/500:  [0.8125, 0.29642946]\n",
      "train 65/500:  [0.75, 0.29648226]\n",
      "train 66/500:  [0.75, 0.29842436]\n",
      "train 67/500:  [0.6875, 0.3059914]\n",
      "train 68/500:  [0.75, 0.30003357]\n",
      "train 69/500:  [0.6875, 0.30366054]\n",
      "train 70/500:  [0.6875, 0.29976326]\n",
      "train 71/500:  [0.75, 0.29812828]\n",
      "train 72/500:  [0.8125, 0.28989053]\n",
      "train 73/500:  [0.8125, 0.28947464]\n",
      "train 74/500:  [0.75, 0.2923559]\n",
      "train 75/500:  [0.75, 0.28817198]\n",
      "train 76/500:  [0.75, 0.29005438]\n",
      "train 77/500:  [0.75, 0.2885841]\n",
      "train 78/500:  [0.75, 0.29179603]\n",
      "train 79/500:  [0.6875, 0.30158648]\n",
      "train 80/500:  [0.6875, 0.29517084]\n",
      "train 81/500:  [0.6875, 0.28830457]\n",
      "train 82/500:  [0.6875, 0.2946955]\n",
      "train 83/500:  [0.75, 0.2856499]\n",
      "train 84/500:  [0.75, 0.28271878]\n",
      "train 85/500:  [0.75, 0.2891478]\n",
      "train 86/500:  [0.75, 0.29157573]\n",
      "train 87/500:  [0.75, 0.28589568]\n",
      "train 88/500:  [0.6875, 0.2909404]\n",
      "train 89/500:  [0.75, 0.29314893]\n",
      "train 90/500:  [0.75, 0.2862716]\n",
      "train 91/500:  [0.75, 0.2861327]\n",
      "train 92/500:  [0.75, 0.27520847]\n",
      "train 93/500:  [0.75, 0.27920783]\n",
      "train 94/500:  [0.8125, 0.2797342]\n",
      "train 95/500:  [0.8125, 0.27480108]\n",
      "train 96/500:  [0.8125, 0.28303197]\n",
      "train 97/500:  [0.75, 0.28347147]\n",
      "train 98/500:  [0.75, 0.28101224]\n",
      "train 99/500:  [0.75, 0.27599168]\n",
      "train 100/500:  [0.75, 0.27209455]\n",
      "train 101/500:  [0.75, 0.27702075]\n",
      "train 102/500:  [0.6875, 0.28053674]\n",
      "train 103/500:  [0.6875, 0.2816956]\n",
      "train 104/500:  [0.75, 0.2782858]\n",
      "train 105/500:  [0.6875, 0.27749527]\n",
      "train 106/500:  [0.75, 0.2778257]\n",
      "train 107/500:  [0.75, 0.2743447]\n",
      "train 108/500:  [0.6875, 0.27992076]\n",
      "train 109/500:  [0.75, 0.27517062]\n",
      "train 110/500:  [0.75, 0.27181578]\n",
      "train 111/500:  [0.6875, 0.28084958]\n",
      "train 112/500:  [0.6875, 0.2856617]\n",
      "train 113/500:  [0.75, 0.27920812]\n",
      "train 114/500:  [0.75, 0.2768237]\n",
      "train 115/500:  [0.6875, 0.2797171]\n",
      "train 116/500:  [0.75, 0.27519095]\n",
      "train 117/500:  [0.75, 0.27111667]\n",
      "train 118/500:  [0.75, 0.275352]\n",
      "train 119/500:  [0.75, 0.2733292]\n",
      "train 120/500:  [0.75, 0.27445203]\n",
      "train 121/500:  [0.6875, 0.28019613]\n",
      "train 122/500:  [0.6875, 0.27505046]\n",
      "train 123/500:  [0.6875, 0.2710185]\n",
      "train 124/500:  [0.6875, 0.2731235]\n",
      "train 125/500:  [0.6875, 0.27928555]\n",
      "train 126/500:  [0.6875, 0.2823639]\n",
      "train 127/500:  [0.6875, 0.27747124]\n",
      "train 128/500:  [0.6875, 0.28356087]\n",
      "train 129/500:  [0.6875, 0.28165075]\n",
      "train 130/500:  [0.9375, 0.26481798]\n",
      "train 131/500:  [0.6875, 0.271551]\n",
      "train 132/500:  [0.6875, 0.27851295]\n",
      "train 133/500:  [0.6875, 0.27520517]\n",
      "train 134/500:  [0.875, 0.2643475]\n",
      "train 135/500:  [0.6875, 0.27116054]\n",
      "train 136/500:  [0.6875, 0.27081716]\n",
      "train 137/500:  [0.6875, 0.27460995]\n",
      "train 138/500:  [0.6875, 0.2814033]\n",
      "train 139/500:  [0.6875, 0.27919024]\n",
      "train 140/500:  [0.6875, 0.27216327]\n",
      "train 141/500:  [0.8125, 0.27281338]\n",
      "train 142/500:  [0.6875, 0.27423775]\n",
      "train 143/500:  [0.75, 0.27263588]\n",
      "train 144/500:  [0.8125, 0.26398754]\n",
      "train 145/500:  [0.75, 0.26802838]\n",
      "train 146/500:  [0.9375, 0.25221416]\n",
      "train 147/500:  [0.875, 0.24921283]\n",
      "train 148/500:  [0.8125, 0.25964472]\n",
      "train 149/500:  [0.75, 0.2539342]\n",
      "train 150/500:  [0.75, 0.25436547]\n",
      "train 151/500:  [0.8125, 0.25454226]\n",
      "train 152/500:  [0.8125, 0.23778573]\n",
      "train 153/500:  [0.8125, 0.25036666]\n",
      "train 154/500:  [0.8125, 0.26269293]\n",
      "train 155/500:  [0.6875, 0.274005]\n",
      "train 156/500:  [0.6875, 0.27495393]\n",
      "train 157/500:  [0.75, 0.27222684]\n",
      "train 158/500:  [0.8125, 0.25369143]\n",
      "train 159/500:  [0.8125, 0.2541812]\n",
      "train 160/500:  [0.8125, 0.25528842]\n",
      "train 161/500:  [0.6875, 0.27796477]\n",
      "train 162/500:  [0.75, 0.26626396]\n",
      "train 163/500:  [0.8125, 0.25493276]\n",
      "train 164/500:  [0.75, 0.25565952]\n",
      "train 165/500:  [0.8125, 0.24752444]\n",
      "train 166/500:  [0.75, 0.25911787]\n",
      "train 167/500:  [0.75, 0.25918713]\n",
      "train 168/500:  [0.75, 0.2627204]\n",
      "train 169/500:  [0.6875, 0.2706682]\n",
      "train 170/500:  [0.75, 0.2738787]\n",
      "train 171/500:  [0.75, 0.26481315]\n",
      "train 172/500:  [0.6875, 0.2689237]\n",
      "train 173/500:  [0.75, 0.26491752]\n",
      "train 174/500:  [0.6875, 0.26857492]\n",
      "train 175/500:  [0.6875, 0.2749552]\n",
      "train 176/500:  [0.75, 0.26577425]\n",
      "train 177/500:  [0.6875, 0.27325466]\n",
      "train 178/500:  [0.6875, 0.27552965]\n",
      "train 179/500:  [0.6875, 0.27336276]\n",
      "train 180/500:  [0.6875, 0.2727683]\n",
      "train 181/500:  [0.75, 0.27164197]\n",
      "train 182/500:  [0.75, 0.2738241]\n",
      "train 183/500:  [0.75, 0.26526663]\n",
      "train 184/500:  [0.6875, 0.27129343]\n",
      "train 185/500:  [0.75, 0.26142868]\n",
      "train 186/500:  [0.75, 0.26791275]\n",
      "train 187/500:  [0.75, 0.25192276]\n",
      "train 188/500:  [0.75, 0.255687]\n",
      "train 189/500:  [0.75, 0.25935167]\n",
      "train 190/500:  [0.75, 0.25469342]\n",
      "train 191/500:  [0.75, 0.25974333]\n",
      "train 192/500:  [0.75, 0.26892233]\n",
      "train 193/500:  [0.75, 0.25907362]\n",
      "train 194/500:  [0.8125, 0.2554581]\n",
      "train 195/500:  [0.6875, 0.26485667]\n",
      "train 196/500:  [0.75, 0.26068312]\n",
      "train 197/500:  [0.6875, 0.27371114]\n",
      "train 198/500:  [0.75, 0.26552564]\n",
      "train 199/500:  [0.6875, 0.26732767]\n",
      "train 200/500:  [0.6875, 0.2720234]\n",
      "train 201/500:  [0.75, 0.2649621]\n",
      "train 202/500:  [0.75, 0.26891688]\n",
      "train 203/500:  [0.75, 0.26193243]\n",
      "train 204/500:  [0.75, 0.26364285]\n",
      "train 205/500:  [0.75, 0.27152675]\n",
      "train 206/500:  [0.75, 0.25340337]\n",
      "train 207/500:  [0.8125, 0.25674364]\n",
      "train 208/500:  [0.6875, 0.26765543]\n",
      "train 209/500:  [0.8125, 0.26126665]\n",
      "train 210/500:  [0.6875, 0.26605576]\n",
      "train 211/500:  [0.6875, 0.2684884]\n",
      "train 212/500:  [0.6875, 0.26589602]\n",
      "train 213/500:  [0.6875, 0.2653814]\n",
      "train 214/500:  [0.6875, 0.26255924]\n",
      "train 215/500:  [0.6875, 0.26332986]\n",
      "train 216/500:  [0.6875, 0.27114582]\n",
      "train 217/500:  [0.9375, 0.25483653]\n",
      "train 218/500:  [1.0, 0.2523063]\n",
      "train 219/500:  [0.75, 0.24881193]\n",
      "train 220/500:  [0.75, 0.2619294]\n",
      "train 221/500:  [0.75, 0.2713138]\n",
      "train 222/500:  [0.75, 0.24956565]\n",
      "train 223/500:  [0.75, 0.24980778]\n",
      "train 224/500:  [0.75, 0.24785244]\n",
      "train 225/500:  [0.75, 0.26786265]\n",
      "train 226/500:  [0.6875, 0.27109748]\n",
      "train 227/500:  [0.9375, 0.25754002]\n",
      "train 228/500:  [0.6875, 0.26329827]\n",
      "train 229/500:  [0.6875, 0.27160782]\n",
      "train 230/500:  [0.6875, 0.26963395]\n",
      "train 231/500:  [0.6875, 0.26736367]\n",
      "train 232/500:  [0.9375, 0.26072297]\n",
      "train 233/500:  [0.9375, 0.26065063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 234/500:  [0.8125, 0.261355]\n",
      "train 235/500:  [0.8125, 0.2603457]\n",
      "train 236/500:  [0.8125, 0.26147807]\n",
      "train 237/500:  [0.6875, 0.2730682]\n",
      "train 238/500:  [0.6875, 0.2724404]\n",
      "train 239/500:  [0.875, 0.26033753]\n",
      "train 240/500:  [0.6875, 0.26709843]\n",
      "train 241/500:  [0.6875, 0.2672644]\n",
      "train 242/500:  [0.6875, 0.2718887]\n",
      "train 243/500:  [0.6875, 0.2784115]\n",
      "train 244/500:  [0.6875, 0.26668894]\n",
      "train 245/500:  [0.6875, 0.2759148]\n",
      "train 246/500:  [0.6875, 0.26443458]\n",
      "train 247/500:  [0.6875, 0.274247]\n",
      "train 248/500:  [0.6875, 0.27983356]\n",
      "train 249/500:  [0.6875, 0.28095835]\n",
      "train 250/500:  [0.6875, 0.27736297]\n",
      "train 251/500:  [0.6875, 0.28025395]\n",
      "train 252/500:  [0.6875, 0.27016342]\n",
      "train 253/500:  [0.875, 0.26248163]\n",
      "train 254/500:  [0.6875, 0.2678308]\n",
      "train 255/500:  [0.6875, 0.27440205]\n",
      "train 256/500:  [0.6875, 0.27473158]\n",
      "train 257/500:  [0.75, 0.26809323]\n",
      "train 258/500:  [0.6875, 0.27841902]\n",
      "train 259/500:  [0.75, 0.2716583]\n",
      "train 260/500:  [0.75, 0.2626897]\n",
      "train 261/500:  [0.9375, 0.26041168]\n",
      "train 262/500:  [0.6875, 0.26776597]\n",
      "train 263/500:  [0.75, 0.26281318]\n",
      "train 264/500:  [0.875, 0.258658]\n",
      "train 265/500:  [0.6875, 0.26545897]\n",
      "train 266/500:  [0.6875, 0.27579004]\n",
      "train 267/500:  [0.6875, 0.27567604]\n",
      "train 268/500:  [0.6875, 0.27154553]\n",
      "train 269/500:  [0.6875, 0.26383734]\n",
      "train 270/500:  [0.8125, 0.26277435]\n",
      "train 271/500:  [0.6875, 0.27075112]\n",
      "train 272/500:  [0.6875, 0.2767312]\n",
      "train 273/500:  [0.6875, 0.27061743]\n",
      "train 274/500:  [0.6875, 0.26588345]\n",
      "train 275/500:  [0.6875, 0.27726603]\n",
      "train 276/500:  [0.6875, 0.27542084]\n",
      "train 277/500:  [0.6875, 0.28254086]\n",
      "train 278/500:  [0.6875, 0.28159323]\n",
      "train 279/500:  [0.6875, 0.2708897]\n",
      "train 280/500:  [0.6875, 0.27287406]\n",
      "train 281/500:  [0.6875, 0.26858366]\n",
      "train 282/500:  [0.6875, 0.27293223]\n",
      "train 283/500:  [0.6875, 0.27030998]\n",
      "train 284/500:  [0.6875, 0.27046037]\n",
      "train 285/500:  [0.6875, 0.26865354]\n",
      "train 286/500:  [0.6875, 0.26724508]\n",
      "train 287/500:  [0.9375, 0.26002938]\n",
      "train 288/500:  [0.6875, 0.26969245]\n",
      "train 289/500:  [0.6875, 0.27541697]\n",
      "train 290/500:  [0.9375, 0.25872916]\n",
      "train 291/500:  [0.6875, 0.26465678]\n",
      "train 292/500:  [0.6875, 0.2712363]\n",
      "train 293/500:  [0.8125, 0.2615233]\n",
      "train 294/500:  [0.6875, 0.26719937]\n",
      "train 295/500:  [0.6875, 0.27098584]\n",
      "train 296/500:  [0.6875, 0.2713942]\n",
      "train 297/500:  [0.6875, 0.27029604]\n",
      "train 298/500:  [0.6875, 0.2677849]\n",
      "train 299/500:  [0.6875, 0.26697892]\n",
      "train 300/500:  [0.6875, 0.27450883]\n",
      "train 301/500:  [0.6875, 0.27638003]\n",
      "train 302/500:  [0.6875, 0.26713702]\n",
      "train 303/500:  [0.8125, 0.2621955]\n",
      "train 304/500:  [0.6875, 0.27036774]\n",
      "train 305/500:  [0.6875, 0.27279425]\n",
      "train 306/500:  [0.8125, 0.26115018]\n",
      "train 307/500:  [0.6875, 0.26483458]\n",
      "train 308/500:  [0.6875, 0.27104878]\n",
      "train 309/500:  [0.6875, 0.2640912]\n",
      "train 310/500:  [0.6875, 0.26562393]\n",
      "train 311/500:  [0.6875, 0.26701564]\n",
      "train 312/500:  [0.6875, 0.26875404]\n",
      "train 313/500:  [0.9375, 0.24620542]\n",
      "train 314/500:  [0.9375, 0.25726014]\n",
      "train 315/500:  [0.9375, 0.25914377]\n",
      "train 316/500:  [0.6875, 0.27264035]\n",
      "train 317/500:  [0.6875, 0.2748]\n",
      "train 318/500:  [0.6875, 0.27713126]\n",
      "train 319/500:  [0.6875, 0.27326882]\n",
      "train 320/500:  [0.6875, 0.2718758]\n",
      "train 321/500:  [0.6875, 0.27607608]\n",
      "train 322/500:  [0.9375, 0.25821954]\n",
      "train 323/500:  [0.9375, 0.25474408]\n",
      "train 324/500:  [0.75, 0.26388615]\n",
      "train 325/500:  [0.6875, 0.27378434]\n",
      "train 326/500:  [0.6875, 0.26971933]\n",
      "train 327/500:  [0.6875, 0.27693194]\n",
      "train 328/500:  [0.6875, 0.2635256]\n",
      "train 329/500:  [0.6875, 0.2656418]\n",
      "train 330/500:  [0.6875, 0.26943403]\n",
      "train 331/500:  [0.6875, 0.2668624]\n",
      "train 332/500:  [0.6875, 0.26959467]\n",
      "train 333/500:  [0.6875, 0.27643192]\n",
      "train 334/500:  [0.6875, 0.28000638]\n",
      "train 335/500:  [0.6875, 0.26360494]\n",
      "train 336/500:  [0.6875, 0.26928377]\n",
      "train 337/500:  [0.6875, 0.26984426]\n",
      "train 338/500:  [0.6875, 0.26955152]\n",
      "train 339/500:  [0.6875, 0.2658643]\n",
      "train 340/500:  [0.8125, 0.26066774]\n",
      "train 341/500:  [0.6875, 0.27156657]\n",
      "train 342/500:  [0.6875, 0.26602787]\n",
      "train 343/500:  [0.6875, 0.27823856]\n",
      "train 344/500:  [0.8125, 0.2606987]\n",
      "train 345/500:  [0.6875, 0.2681984]\n",
      "train 346/500:  [0.8125, 0.26095635]\n",
      "train 347/500:  [0.6875, 0.2713089]\n",
      "train 348/500:  [0.6875, 0.26781407]\n",
      "train 349/500:  [0.6875, 0.2841274]\n",
      "train 350/500:  [0.6875, 0.2767535]\n",
      "train 351/500:  [0.9375, 0.25372404]\n",
      "train 352/500:  [0.9375, 0.25287417]\n",
      "train 353/500:  [0.6875, 0.26377478]\n",
      "train 354/500:  [0.6875, 0.2746169]\n",
      "train 355/500:  [0.6875, 0.2673188]\n",
      "train 356/500:  [0.75, 0.2659448]\n",
      "train 357/500:  [0.6875, 0.2725461]\n",
      "train 358/500:  [0.6875, 0.27416036]\n",
      "train 359/500:  [0.6875, 0.273399]\n",
      "train 360/500:  [0.6875, 0.26588422]\n",
      "train 361/500:  [0.6875, 0.2732945]\n",
      "train 362/500:  [0.6875, 0.27298635]\n",
      "train 363/500:  [0.75, 0.26490775]\n",
      "train 364/500:  [0.9375, 0.25383013]\n",
      "train 365/500:  [0.6875, 0.2654699]\n",
      "train 366/500:  [1.0, 0.25845218]\n",
      "train 367/500:  [0.6875, 0.26502016]\n",
      "train 368/500:  [0.8125, 0.26067376]\n",
      "train 369/500:  [0.6875, 0.26330498]\n",
      "train 370/500:  [0.9375, 0.2590467]\n",
      "train 371/500:  [0.6875, 0.26349896]\n",
      "train 372/500:  [0.6875, 0.26448327]\n",
      "train 373/500:  [0.875, 0.26005843]\n",
      "train 374/500:  [0.9375, 0.24959356]\n",
      "train 375/500:  [0.6875, 0.267002]\n",
      "train 376/500:  [0.6875, 0.26797813]\n",
      "train 377/500:  [0.6875, 0.2687231]\n",
      "train 378/500:  [0.6875, 0.26234132]\n",
      "train 379/500:  [0.6875, 0.2658706]\n",
      "train 380/500:  [0.6875, 0.27282062]\n",
      "train 381/500:  [0.6875, 0.26889342]\n",
      "train 382/500:  [0.6875, 0.26881683]\n",
      "train 383/500:  [0.6875, 0.27710134]\n",
      "train 384/500:  [0.6875, 0.26816398]\n",
      "train 385/500:  [0.6875, 0.2654345]\n",
      "train 386/500:  [0.6875, 0.2660706]\n",
      "train 387/500:  [0.875, 0.26137137]\n",
      "train 388/500:  [0.875, 0.25932044]\n",
      "train 389/500:  [0.875, 0.2599925]\n",
      "train 390/500:  [0.9375, 0.25287175]\n",
      "train 391/500:  [0.875, 0.2612015]\n",
      "train 392/500:  [0.9375, 0.25411522]\n",
      "train 393/500:  [0.6875, 0.2667281]\n",
      "train 394/500:  [0.6875, 0.27266464]\n",
      "train 395/500:  [0.6875, 0.26627243]\n",
      "train 396/500:  [0.6875, 0.26292616]\n",
      "train 397/500:  [0.6875, 0.2664796]\n",
      "train 398/500:  [0.6875, 0.26764682]\n",
      "train 399/500:  [0.6875, 0.2742219]\n",
      "train 400/500:  [0.6875, 0.27285695]\n",
      "train 401/500:  [0.6875, 0.26232493]\n",
      "train 402/500:  [0.6875, 0.2657666]\n",
      "train 403/500:  [0.6875, 0.26728243]\n",
      "train 404/500:  [0.875, 0.2592076]\n",
      "train 405/500:  [0.8125, 0.26142833]\n",
      "train 406/500:  [0.6875, 0.2690323]\n",
      "train 407/500:  [0.6875, 0.28046036]\n",
      "train 408/500:  [0.6875, 0.27254412]\n",
      "train 409/500:  [0.6875, 0.2654115]\n",
      "train 410/500:  [0.6875, 0.268968]\n",
      "train 411/500:  [0.6875, 0.27258152]\n",
      "train 412/500:  [0.75, 0.26112214]\n",
      "train 413/500:  [0.6875, 0.27100167]\n",
      "train 414/500:  [0.6875, 0.27123377]\n",
      "train 415/500:  [0.6875, 0.2684941]\n",
      "train 416/500:  [0.6875, 0.2706142]\n",
      "train 417/500:  [0.6875, 0.2715878]\n",
      "train 418/500:  [0.6875, 0.2776956]\n",
      "train 419/500:  [0.6875, 0.2783807]\n",
      "train 420/500:  [0.6875, 0.27406567]\n",
      "train 421/500:  [0.75, 0.26474985]\n",
      "train 422/500:  [0.6875, 0.2716727]\n",
      "train 423/500:  [0.6875, 0.27235124]\n",
      "train 424/500:  [0.6875, 0.27090466]\n",
      "train 425/500:  [0.75, 0.26924795]\n",
      "train 426/500:  [0.6875, 0.2721331]\n",
      "train 427/500:  [0.6875, 0.27688244]\n",
      "train 428/500:  [0.6875, 0.2766863]\n",
      "train 429/500:  [0.75, 0.26524585]\n",
      "train 430/500:  [0.8125, 0.2647668]\n",
      "train 431/500:  [0.6875, 0.27096468]\n",
      "train 432/500:  [0.9375, 0.25813898]\n",
      "train 433/500:  [0.75, 0.26595247]\n",
      "train 434/500:  [0.9375, 0.25743425]\n",
      "train 435/500:  [0.9375, 0.25457248]\n",
      "train 436/500:  [0.6875, 0.26325577]\n",
      "train 437/500:  [0.9375, 0.25373054]\n",
      "train 438/500:  [0.9375, 0.25414887]\n",
      "train 439/500:  [0.6875, 0.27363157]\n",
      "train 440/500:  [0.6875, 0.2649836]\n",
      "train 441/500:  [0.6875, 0.26416552]\n",
      "train 442/500:  [0.6875, 0.26575133]\n",
      "train 443/500:  [0.6875, 0.27046794]\n",
      "train 444/500:  [0.6875, 0.2694101]\n",
      "train 445/500:  [0.75, 0.26202774]\n",
      "train 446/500:  [0.6875, 0.2629404]\n",
      "train 447/500:  [0.6875, 0.26174036]\n",
      "train 448/500:  [0.6875, 0.26854867]\n",
      "train 449/500:  [0.9375, 0.2576963]\n",
      "train 450/500:  [0.6875, 0.2692265]\n",
      "train 451/500:  [0.6875, 0.2739789]\n",
      "train 452/500:  [0.6875, 0.27277926]\n",
      "train 453/500:  [0.6875, 0.26658115]\n",
      "train 454/500:  [0.75, 0.26211584]\n",
      "train 455/500:  [0.6875, 0.2735381]\n",
      "train 456/500:  [0.9375, 0.259457]\n",
      "train 457/500:  [1.0, 0.2601307]\n",
      "train 458/500:  [0.6875, 0.2777794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 459/500:  [0.6875, 0.2753654]\n",
      "train 460/500:  [0.6875, 0.26638442]\n",
      "train 461/500:  [0.6875, 0.27224737]\n",
      "train 462/500:  [0.6875, 0.27342492]\n",
      "train 463/500:  [0.6875, 0.26373065]\n",
      "train 464/500:  [0.9375, 0.25627273]\n",
      "train 465/500:  [0.75, 0.26092836]\n",
      "train 466/500:  [0.6875, 0.2659598]\n",
      "train 467/500:  [0.6875, 0.26901832]\n",
      "train 468/500:  [0.6875, 0.2699873]\n",
      "train 469/500:  [0.6875, 0.26697087]\n",
      "train 470/500:  [0.6875, 0.27327275]\n",
      "train 471/500:  [0.6875, 0.26382732]\n",
      "train 472/500:  [0.6875, 0.2757343]\n",
      "train 473/500:  [0.6875, 0.27055293]\n",
      "train 474/500:  [0.6875, 0.2702996]\n",
      "train 475/500:  [0.9375, 0.25451174]\n",
      "train 476/500:  [0.6875, 0.26605788]\n",
      "train 477/500:  [0.6875, 0.27343285]\n",
      "train 478/500:  [0.6875, 0.26310217]\n",
      "train 479/500:  [0.6875, 0.26542437]\n",
      "train 480/500:  [0.6875, 0.2615328]\n",
      "train 481/500:  [0.6875, 0.26810747]\n",
      "train 482/500:  [0.6875, 0.27055216]\n",
      "train 483/500:  [0.6875, 0.27959013]\n",
      "train 484/500:  [0.6875, 0.27411544]\n",
      "train 485/500:  [0.6875, 0.26483142]\n",
      "train 486/500:  [0.6875, 0.27137336]\n",
      "train 487/500:  [0.6875, 0.26365247]\n",
      "train 488/500:  [0.6875, 0.26863107]\n",
      "train 489/500:  [0.6875, 0.26755294]\n",
      "train 490/500:  [0.6875, 0.27221787]\n",
      "train 491/500:  [0.6875, 0.26739365]\n",
      "train 492/500:  [0.6875, 0.26371455]\n",
      "train 493/500:  [0.6875, 0.27046585]\n",
      "train 494/500:  [0.6875, 0.26287934]\n",
      "train 495/500:  [0.6875, 0.27725315]\n",
      "train 496/500:  [0.6875, 0.2735728]\n",
      "train 497/500:  [0.6875, 0.26810384]\n",
      "train 498/500:  [0.6875, 0.28443545]\n",
      "train 499/500:  [0.6875, 0.26719314]\n",
      "test:  0.800628662109375\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(epoch):\n",
    "        for i in range(batch_num):\n",
    "            batch_xs = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_ys = train_y[i*batch_size:(i+1)*batch_size]\n",
    "            sess.run(op, feed_dict={x: batch_xs,y: batch_ys,keep_prob:0.3})\n",
    "            if i == batch_num -1:\n",
    "                print('train %d/%d: '%(step,epoch), sess.run([accuracy,loss], feed_dict={x: batch_xs,y: batch_ys,keep_prob:1.0}))\n",
    "    \n",
    "    acc = 0\n",
    "    batch_num = len(test_x)//batch_size\n",
    "    for i in range(batch_num):\n",
    "            batch_xs = test_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_ys = test_y[i*batch_size:(i+1)*batch_size]\n",
    "            acc = (acc + sess.run(accuracy, feed_dict={x:batch_xs,y:batch_ys,keep_prob:1.0}))/2\n",
    "    print('test: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
